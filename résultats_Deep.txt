--------Premier essai : ----------------------
Labels limits : 20-40-60-80
model = Sequential([
    Dense(32, input_dim=100),
    Activation  ('relu'),
    Dense (5),
    Activation('softmax'),
])

model.compile(loss='categorical_crossentropy',
              optimizer='adam',
              metrics=['accuracy'])

Score final : [0.31392840335243627, 0.9111841917037964]
15s/epoch

--------2ème essai : -------------------------
Labels limits : 20-40-60-80
model = Sequential([
    Dense(32, input_dim=100),
    Activation  ('relu'),
    Dense(64, activation='relu'),
    Dropout(0.5),
    Dense (5),
    Activation('softmax'),
])

model.compile(loss='categorical_crossentropy',
              optimizer='adam',
              metrics=['accuracy'])

Score final : [0.1831302536945594, 0.9375]
35s/epoch
------------------------------------------------------
seed 124 : Score final : [0.18532493607582232, 0.9375]
------------------------------------------------------

Same on D_C : (seed 123)
Score final : [0.030761849331228358, 0.9934210777282715] (Very good results!!!!!)

------------------------------------------------------
------------------------------------------------------

Données 2
model = Sequential([
    Dense(32, input_dim=100),
    Activation  ('relu'),
    Dense(64, activation='relu'),
    Dropout(0.2),
    Dense (5),
    Activation('softmax'),
])

#Compile model
model.compile(loss='categorical_crossentropy',
              optimizer='adam',
              metrics=['accuracy'])
15 epochs
Score final : [0.09485191212468773, 0.9470376968383789]



Tests individuels : 
[1 2 2 2 2 0 0 1 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2
 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2
 2 2 2 2 2 2 2 2 2 2 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 4]

Rsultats précis : 
[[1.78465977e-01 3.54315877e-01 2.40953729e-01 1.51794329e-01 7.44700059e-02]
 [6.80871978e-02 3.18658799e-01 4.41445172e-01 1.58916354e-01 1.28924306e-02]
 [6.83566704e-02 1.66045442e-01 4.95962590e-01 2.55082488e-01 1.45528149e-02]
 [1.01656862e-01 1.99167952e-01 3.74039739e-01 3.06871355e-01 1.82640944e-02]
 [3.67134325e-02 1.50335014e-01 7.67415762e-01 4.21900600e-02 3.34576052e-03]
 [2.71024287e-01 2.27147698e-01 2.05391511e-01 2.22335950e-01 7.41006136e-02]
 [4.39174533e-01 3.15307289e-01 1.21650368e-01 1.10680856e-01 1.31869484e-02]
 [3.87139209e-02 9.10771191e-01 2.88861170e-02 1.99554283e-02 1.67346466e-03]
 [4.42941859e-02 5.94072826e-02 5.98048270e-01 2.91128844e-01 7.12148054e-03]
 [5.67475287e-03 9.91067350e-01 1.82652695e-03 1.20815495e-03 2.23241863e-04]
 [1.04169513e-03 9.98766422e-01 8.02463910e-05 9.51715265e-05 1.65193360e-05]
 [4.37519513e-03 9.93067622e-01 5.83447516e-04 1.62347220e-03 3.50287941e-04]
 [1.08170221e-02 9.78762031e-01 7.84097612e-03 2.14040256e-03 4.39511990e-04]
 [8.66189599e-03 9.88193333e-01 1.45175646e-03 1.15259504e-03 5.40502078e-04]
 [2.11607548e-03 9.97230709e-01 3.03163542e-04 2.96023063e-04 5.41201225e-05]
 [3.17913387e-03 9.95407999e-01 5.53152815e-04 7.79070659e-04 8.06199605e-05]
 [6.82066986e-03 9.86030042e-01 3.48920329e-03 3.40151787e-03 2.58655025e-04]
 [2.65201204e-03 9.96438026e-01 5.54189901e-04 2.77013838e-04 7.87036406e-05]
 [8.63237772e-03 9.88319635e-01 7.64073629e-04 1.71833311e-03 5.65688242e-04]
 [7.83199037e-04 9.98975992e-01 7.56268855e-05 1.47545463e-04 1.75873756e-05]]
